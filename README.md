MOSDAC/ISRO RAG Pipeline API
(Retrieval-Augmented Generation + Knowledge Graph + Whisper voice support)

ğŸ“– Overview
This project is a FastAPI-based web service that provides an AI-powered assistant for MOSDAC/ISRO data.
It combines retrieval-augmented generation (RAG), knowledge graph (KG) extraction, and voice query processing (via Whisper).
The API exposes endpoints for text and voice queries, returning rich answers and visual KG graphs.

âœ¨ Features
ğŸ” RAG pipeline over MOSDAC/ISRO documents.

ğŸ—£ï¸ Supports text and voice queries (recorded live or uploaded).

ğŸ“Š Extracts entities from answers and generates knowledge graph images.

ğŸŒ RESTful HTTP API with JSON and image responses.

ğŸ”„ Cross-Origin Resource Sharing (CORS) enabled.

ğŸ” Health-check endpoint.

ğŸ“¡ Endpoints
Method	Endpoint	Description
GET	/	Root: API version, status, endpoints list
GET	/health	Health check: verify models and data
POST	/rag	Main RAG pipeline: text or voice
POST	/rag/voice-upload	Upload audio file (wav, mp3) and process
GET	/kg-image/{filename}	Download KG image
GET	/kg-image/{filename}/base64	Get KG image as base64

ğŸ§ª Example Usage
Run a text query
bash
Copy
Edit
POST /rag
mode=text
query=What is the function of INSAT-3D?
Run a voice query (record live)
bash
Copy
Edit
POST /rag
mode=voice
duration=5
Upload an audio file
bash
Copy
Edit
POST /rag/voice-upload
audio=@query.wav
Get KG image
arduino
Copy
Edit
GET /kg-image/knowledge_graph_INSAT_3D.png
Or as base64:

bash
Copy
Edit
GET /kg-image/knowledge_graph_INSAT_3D.png/base64
ğŸ—ï¸ Technology Stack
FastAPI â€“ web server framework

LangChain â€“ RAG pipeline

Google Gemini â€“ LLM for answering

HuggingFace Sentence Transformers â€“ embeddings

ChromaDB â€“ vector store

OpenAI Whisper â€“ speech-to-text

spaCy + NLTK â€“ entity extraction

NetworkX + Matplotlib â€“ KG visualization

ğŸ› ï¸ Setup
Prerequisites
Python 3.9+

ffmpeg installed and added to PATH (for Whisper audio)

Install dependencies
bash
Copy
Edit
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
Run the server
bash
Copy
Edit
uvicorn app:app --reload --host 0.0.0.0 --port 8000
The server will start on:
ğŸ‘‰ http://localhost:8000

